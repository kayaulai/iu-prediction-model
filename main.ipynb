{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# set the random seed for reproduction\n",
    "SEED=190\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "#checking if GPU is available or not\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Split into Train and Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_category(df):\n",
    "    data_category = {}\n",
    "    for col in df.columns:\n",
    "        if df.dtypes[[col]][0] == object:\n",
    "            data_category[col] = {}\n",
    "            for (i, cat) in enumerate(df[[col]].drop_duplicates().values.flatten().tolist()):\n",
    "                data_category[col][cat] = i\n",
    "    \n",
    "    return data_category\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('predict_df.csv', low_memory=False)\n",
    "data_category = get_category(df)\n",
    "\n",
    "msk = np.random.rand(len(df)) > 0.2\n",
    "train_df = df[msk]\n",
    "valid_df = df[~msk]\n",
    "# train_df.to_csv('sbc_train.csv', index=False)\n",
    "# valid_df.to_csv('sbc_valid.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset for Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import Dataset\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "class SBCDataset(Dataset):\n",
    "    def __init__(self, raw_data, cat_dict: dict, predictors: List[str], response: str, inference=False):\n",
    "        self.cat_dict = cat_dict\n",
    "        self.pred_data = self.one_hot(self.encode(raw_data, cat_dict, predictors))\n",
    "        self.labels = raw_data[[response]].to_numpy(dtype=float)\n",
    "        self.inference = inference\n",
    "        self.shape = self.pred_data.shape\n",
    "\n",
    "    def encode(self, raw_data, cat_dict, predictors):\n",
    "        data = pd.DataFrame()\n",
    "        for col in predictors:\n",
    "            if \"currLength\" not in col and \":\" not in col:\n",
    "                data[[col]] = raw_data[[col]].replace(cat_dict)\n",
    "            else: \n",
    "                data[[col]] = raw_data[[col]]\n",
    "        return data\n",
    "    \n",
    "    def one_hot(self, raw_data):\n",
    "        data = np.zeros(shape=(len(raw_data), 1))\n",
    "        for col in raw_data.columns:\n",
    "            if \"currLength\" not in col and \":\" not in col:\n",
    "                data = np.append(data, \n",
    "                                pd.get_dummies(raw_data[[col]], drop_first=True, dtype=float), \n",
    "                                axis=1)\n",
    "            elif \":\" in col:\n",
    "                temp_col = col.split(\":\")\n",
    "                data = np.append(data, \n",
    "                                pd.get_dummies(raw_data[[temp_col[0]]], drop_first=True, dtype=float).to_numpy() * raw_data[[temp_col[1]]].to_numpy(), \n",
    "                                axis=1)\n",
    "            else: \n",
    "                data = np.append(data, raw_data[[col]], axis=1)\n",
    "        return data[:, 1:]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pred_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if not self.inference:\n",
    "            label = self.labels[idx]\n",
    "            return torch.tensor(self.pred_data[idx], dtype=torch.float), torch.tensor(label, dtype=torch.float)\n",
    "        else:\n",
    "            return torch.tensor(self.pred_data[idx], dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2048\n",
    "TEST_BATCH_SIZE = 2048\n",
    "\n",
    "# create the dataset\n",
    "predictors = [\"prevEndnote\", \"prev1\", \"prev2\", \"prev3\", \"currLength\", \"init1\", \"init2\", \"init3\"]\n",
    "train_ds = SBCDataset(train_df, cat_dict=data_category, predictors=predictors, response=\"preBoundary\")\n",
    "valid_ds = SBCDataset(valid_df, cat_dict=data_category, predictors=predictors, response=\"preBoundary\")\n",
    "\n",
    "# build the dataloader\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_ds, batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    valid_ds, batch_size=TEST_BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define the Training and Validation Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self, model, criterion, lr_rate, max_epoch):\n",
    "        self.model = model.to(device)\n",
    "        self.criterion = criterion.to(device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr_rate)\n",
    "        self.max_epoch = max_epoch\n",
    "\n",
    "    def run(self, train_loader, valid_loader):\n",
    "        # calculate the inital loss and accu on validation set\n",
    "        valid_best_loss = self.validate(-1, valid_loader, best_loss=None)\n",
    "        for epoch in range(self.max_epoch):\n",
    "            self.train(epoch, train_loader)\n",
    "            # save the checkpoint with the lowest validation loss\n",
    "            valid_best_loss = self.validate(epoch, valid_loader, valid_best_loss)\n",
    "\n",
    "    def train(self, epoch, loader):            \n",
    "        self.model.train()\n",
    "        running_loss, total, correct = 0.0, 0, 0\n",
    "        with tqdm(enumerate(loader, 0), mininterval=10) as tepoch:\n",
    "            for i, data in tepoch:\n",
    "                # get the inputs; data is a list of [inputs, labels]\n",
    "                # inputs: tensor, (batch_size, predictors_size)\n",
    "                # labels: tensor, (batch_size, 1)\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                # calculate the metric\n",
    "                match, number = self.cal_metric(outputs.data, labels)\n",
    "\n",
    "                # gather statistics\n",
    "                total += number\n",
    "                correct += match\n",
    "                running_loss += loss.item()\n",
    "                tepoch.set_postfix(loss=loss.item(), accuracy=100. * correct / total)\n",
    "\n",
    "        running_loss /= len(loader)\n",
    "        \n",
    "        print('Training | Epoch: {}| Loss: {:.3f} | Accuracy on train: {:.1f}%'.format \\\n",
    "              (epoch+1, running_loss, 100 * correct / total))\n",
    "\n",
    "    def validate(self, epoch, loader, best_loss=None):\n",
    "        # switch to the evaluation mode, do not need to calculate the gradient\n",
    "        self.model.eval()\n",
    "        running_loss, total, correct = 0.0, 0, 0\n",
    "        for i, data in tqdm(enumerate(loader)):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # replace the outputs and loss\n",
    "            outputs = self.model(inputs)\n",
    "            loss = self.criterion(outputs, labels)\n",
    "\n",
    "            # calculate the metric\n",
    "            match, number = self.cal_metric(outputs.data, labels)\n",
    "\n",
    "            # gather statistics\n",
    "            total += number\n",
    "            correct += match\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        running_loss /= len(loader)\n",
    "\n",
    "        if best_loss is None or running_loss < best_loss:\n",
    "            # if a better loss appears, save the checkpoint\n",
    "            save_file = 'best_epoch{}_loss{:.2f}_accu{:.2f}.pt'.format(epoch+1, running_loss, 100 * correct / total)\n",
    "            print('Save to file: ', save_file)\n",
    "            torch.save(self.model, save_file)\n",
    "\n",
    "            # overwrite the best_checkpoint.pt file\n",
    "            torch.save(self.model, 'best_checkpoint.pt')\n",
    "\n",
    "            best_loss = running_loss\n",
    "\n",
    "        print('Validation | Epoch: {}| Loss: {:.3f} | Accuracy on val: {:.1f}%'.format \\\n",
    "              (epoch+1, running_loss,100 * correct / total))\n",
    "\n",
    "        return best_loss\n",
    "\n",
    "\n",
    "    def cal_metric(self, outputs, labels):\n",
    "        # compare predictions to ground truth\n",
    "        _, predicted = torch.max(outputs, 1, keepdim=True)\n",
    "        number = labels.size(0)\n",
    "        correct = (predicted == labels).sum().item()\n",
    "        return correct, number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define the Model Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim: int):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 1)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCH = 20\n",
    "LEARNING_RATE = 0.001\n",
    "input_shape = train_ds.shape\n",
    "model = MLP(input_shape[-1])\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "trainer = Trainer(model, criterion, LEARNING_RATE, max_epoch=NUM_EPOCH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
