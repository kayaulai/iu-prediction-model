{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# set the random seed for reproduction\n",
    "SEED=190\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "#checking if GPU is available or not\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Split into Train and Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_category(df):\n",
    "    data_category = {}\n",
    "    for col in df.columns:\n",
    "        if df.dtypes[[col]][0] == object:\n",
    "            data_category[col] = {}\n",
    "            for (i, cat) in enumerate(df[[col]].drop_duplicates().values.flatten().tolist()):\n",
    "                data_category[col][cat] = i\n",
    "    \n",
    "    return data_category\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('predict_df.csv', low_memory=False)\n",
    "data_category = get_category(df)\n",
    "\n",
    "msk = np.random.rand(len(df)) > 0.2\n",
    "train_df = df[msk]\n",
    "valid_df = df[~msk]\n",
    "# train_df.to_csv('sbc_train.csv', index=False)\n",
    "# valid_df.to_csv('sbc_valid.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset for Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import Dataset\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "class SBCDataset(Dataset):\n",
    "    def __init__(self, raw_data, cat_dict: dict, predictors: List[str], response: str, inference=False):\n",
    "        self.cat_dict = cat_dict\n",
    "        self.pred_data = self.one_hot(raw_data, cat_dict, predictors)\n",
    "        self.labels = raw_data[[response]].to_numpy(dtype=float)\n",
    "        self.inference = inference\n",
    "        self.shape = self.pred_data.shape\n",
    "\n",
    "    def one_hot(self, raw_data, cat_dict, predictors):\n",
    "        data = np.zeros(shape=(len(raw_data), 1))\n",
    "        for col in predictors:\n",
    "            if \"currLength\" not in col and \":\" not in col:\n",
    "                data = np.append(data, \n",
    "                                raw_data[[col]].replace(cat_dict).to_numpy(), \n",
    "                                axis=1)\n",
    "            elif \":\" in col:\n",
    "                temp_col = col.split(\":\")\n",
    "                data = np.append(data, \n",
    "                                raw_data[[temp_col[0]]].replace(cat_dict).to_numpy() * raw_data[[temp_col[1]]].to_numpy(), \n",
    "                                axis=1)\n",
    "            else: \n",
    "                data = np.append(data, raw_data[[col]].to_numpy(dtype=float), axis=1)\n",
    "        return data[:, 1:]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pred_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if not self.inference:\n",
    "            label = self.labels[idx]\n",
    "            return torch.tensor(self.pred_data[idx], dtype=torch.float), torch.tensor(label, dtype=torch.float)\n",
    "        else:\n",
    "            return torch.tensor(self.pred_data[idx], dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "TEST_BATCH_SIZE = 1024\n",
    "\n",
    "# create the dataset\n",
    "predictors = [\"prevEndnote\", \"prev1\", \"prev2\", \"prev3\", \"currLength\", \"init1\", \"init2\", \"init3\"]\n",
    "train_ds = SBCDataset(train_df, cat_dict=data_category, predictors=predictors, response=\"preBoundary\")\n",
    "valid_ds = SBCDataset(valid_df, cat_dict=data_category, predictors=predictors, response=\"preBoundary\")\n",
    "\n",
    "# build the dataloader\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_ds, batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    valid_ds, batch_size=TEST_BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define the Training and Validation Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self, model, criterion, optimizer, max_epoch, scheduler=None, multi_optimizer=None):\n",
    "        self.model = model.to(device)\n",
    "        self.criterion = criterion.to(device)\n",
    "        self.optimizer = optimizer\n",
    "        self.multi_optimizer = None\n",
    "        if multi_optimizer: self.multi_optimizer = multi_optimizer\n",
    "        self.max_epoch = max_epoch\n",
    "        self.scheduler = scheduler\n",
    "\n",
    "    def run(self,train_loader, valid_loader):\n",
    "        # calculate the inital loss and accu on validation set\n",
    "        valid_best_loss = self.validate(-1, valid_loader, best_loss=None)\n",
    "        for epoch in range(self.max_epoch):\n",
    "            self.train(epoch, train_loader)\n",
    "            # save the checkpoint with the lowest validation loss\n",
    "            valid_best_loss = self.validate(epoch, valid_loader, valid_best_loss)\n",
    "\n",
    "    def train(self, epoch, loader):\n",
    "        # Update optimizer\n",
    "        if self.multi_optimizer:\n",
    "            if self.multi_optimizer(epoch) != self.optimizer:\n",
    "                self.optimizer = self.multi_optimizer(epoch)\n",
    "                self.optimizer.step()\n",
    "\n",
    "            self.optimizer = self.multi_optimizer(epoch)\n",
    "            \n",
    "        # Updated: Using custom defined scheduler (Modified from \"Dive into Deep Learning\")\n",
    "        if self.scheduler:\n",
    "            if self.scheduler.__module__ == lr_scheduler.__name__:\n",
    "                # Using PyTorch In-Built scheduler\n",
    "                scheduler.step()\n",
    "            else:\n",
    "                # Using custom defined scheduler\n",
    "                for param_group in self.optimizer.param_groups:\n",
    "                    param_group['lr'] = self.scheduler(epoch) \n",
    "            \n",
    "        # switch to the evaluation mode, do not calculate the gradient\n",
    "        self.model.train()\n",
    "        running_loss, total, correct = 0.0, 0, 0\n",
    "        with tqdm(enumerate(loader, 0), mininterval=10) as tepoch:\n",
    "            for i, data in tepoch:\n",
    "                # get the inputs; data is a list of [inputs, labels]\n",
    "                # inputs: tensor, (batch_size, image_size, image_size)\n",
    "                # labels: tensor, (batch_size, 1)\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                # calculate the metric\n",
    "                match, number = self.cal_metric(outputs.data, labels)\n",
    "\n",
    "                # gather statistics\n",
    "                total += number\n",
    "                correct += match\n",
    "                running_loss += loss.item()\n",
    "                tepoch.set_postfix(loss=loss.item(), accuracy=100. * correct / total)\n",
    "\n",
    "        running_loss /= len(loader)\n",
    "        \n",
    "        print('Training | Epoch: {}| Loss: {:.3f} | Accuracy on train: {:.1f}'.format \\\n",
    "              (epoch+1, running_loss, 100 * correct / total))\n",
    "\n",
    "    def validate(self, epoch, loader, best_loss=None):\n",
    "        # switch to the evaluation mode, do not need to calculate the gradient\n",
    "        self.model.eval()\n",
    "        running_loss, total, correct = 0.0, 0, 0\n",
    "        for i, data in tqdm(enumerate(loader)):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # UPDATED\n",
    "            # replace the outputs and loss\n",
    "            outputs = self.model(inputs)\n",
    "            loss = self.criterion(outputs, labels)\n",
    "\n",
    "            # calculate the metric\n",
    "            match, number = self.cal_metric(outputs.data, labels)\n",
    "\n",
    "            # gather statistics\n",
    "            total += number\n",
    "            correct += match\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        running_loss /= len(loader)\n",
    "\n",
    "        if best_loss is None or running_loss < best_loss:\n",
    "            # if a better loss appears, save the checkpoint\n",
    "            save_file = 'best_epoch{}_loss{:.2f}_accu{:.2f}.pt'.format(epoch+1, running_loss, 100 * correct / total)\n",
    "            print('Save to file: ', save_file)\n",
    "            torch.save(self.model, save_file)\n",
    "\n",
    "            # overwrite the best_checkpoint.pt file\n",
    "            torch.save(self.model, 'best_checkpoint.pt')\n",
    "\n",
    "            best_loss = running_loss\n",
    "\n",
    "        print('Validation | Epoch: {}| Loss: {:.3f} | Accuracy on val: {:.1f}'.format \\\n",
    "              (epoch+1, running_loss,100 * correct / total))\n",
    "\n",
    "        return best_loss\n",
    "\n",
    "\n",
    "    def cal_metric(self, outputs, labels):\n",
    "        # compare predictions to ground truth\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        number = labels.size(0)\n",
    "        correct = (predicted == labels).sum().item()\n",
    "        return correct, number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define the Model Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "        Multilayer perceptron network\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 1)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "NUM_EPOCH = 10\n",
    "LEARNING_RATE = 0.1\n",
    "input_shape = train_ds.shape\n",
    "model = MLP(input_shape[-1])\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "trainer = Trainer(model, criterion, optimizer, max_epoch=NUM_EPOCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "52it [00:00, 95.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save to file:  best_epoch0_loss0.00_accu76998.75.pt\n",
      "Validation | Epoch: 0| Loss: 0.000 | Accuracy on val: 76998.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1013it [00:03, 289.18it/s, accuracy=4.85e+3, loss=-]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\tianr\\OneDrive\\桌面\\6_nn_singlepred_fit\\main.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/tianr/OneDrive/%E6%A1%8C%E9%9D%A2/6_nn_singlepred_fit/main.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39mrun(train_loader, valid_loader)\n",
      "\u001b[1;32mc:\\Users\\tianr\\OneDrive\\桌面\\6_nn_singlepred_fit\\main.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tianr/OneDrive/%E6%A1%8C%E9%9D%A2/6_nn_singlepred_fit/main.ipynb#X20sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m valid_best_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalidate(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, valid_loader, best_loss\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tianr/OneDrive/%E6%A1%8C%E9%9D%A2/6_nn_singlepred_fit/main.ipynb#X20sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_epoch):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/tianr/OneDrive/%E6%A1%8C%E9%9D%A2/6_nn_singlepred_fit/main.ipynb#X20sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain(epoch, train_loader)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tianr/OneDrive/%E6%A1%8C%E9%9D%A2/6_nn_singlepred_fit/main.ipynb#X20sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39m# save the checkpoint with the lowest validation loss\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tianr/OneDrive/%E6%A1%8C%E9%9D%A2/6_nn_singlepred_fit/main.ipynb#X20sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     valid_best_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalidate(epoch, valid_loader, valid_best_loss)\n",
      "\u001b[1;32mc:\\Users\\tianr\\OneDrive\\桌面\\6_nn_singlepred_fit\\main.ipynb Cell 14\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tianr/OneDrive/%E6%A1%8C%E9%9D%A2/6_nn_singlepred_fit/main.ipynb#X20sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m         correct \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m match\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tianr/OneDrive/%E6%A1%8C%E9%9D%A2/6_nn_singlepred_fit/main.ipynb#X20sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m         running_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/tianr/OneDrive/%E6%A1%8C%E9%9D%A2/6_nn_singlepred_fit/main.ipynb#X20sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m         tepoch\u001b[39m.\u001b[39mset_postfix(loss\u001b[39m=\u001b[39mloss\u001b[39m.\u001b[39mitem(), accuracy\u001b[39m=\u001b[39m\u001b[39m100.\u001b[39m \u001b[39m*\u001b[39m correct \u001b[39m/\u001b[39m total)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tianr/OneDrive/%E6%A1%8C%E9%9D%A2/6_nn_singlepred_fit/main.ipynb#X20sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m running_loss \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(loader)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tianr/OneDrive/%E6%A1%8C%E9%9D%A2/6_nn_singlepred_fit/main.ipynb#X20sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTraining | Epoch: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m| Loss: \u001b[39m\u001b[39m{:.3f}\u001b[39;00m\u001b[39m | Accuracy on train: \u001b[39m\u001b[39m{:.1f}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat \\\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tianr/OneDrive/%E6%A1%8C%E9%9D%A2/6_nn_singlepred_fit/main.ipynb#X20sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m       (epoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, running_loss, \u001b[39m100\u001b[39m \u001b[39m*\u001b[39m correct \u001b[39m/\u001b[39m total))\n",
      "File \u001b[1;32mc:\\Users\\tianr\\anaconda3\\Lib\\site-packages\\tqdm\\std.py:1428\u001b[0m, in \u001b[0;36mtqdm.set_postfix\u001b[1;34m(self, ordered_dict, refresh, **kwargs)\u001b[0m\n\u001b[0;32m   1425\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpostfix \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(key \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m postfix[key]\u001b[39m.\u001b[39mstrip()\n\u001b[0;32m   1426\u001b[0m                          \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m postfix\u001b[39m.\u001b[39mkeys())\n\u001b[0;32m   1427\u001b[0m \u001b[39mif\u001b[39;00m refresh:\n\u001b[1;32m-> 1428\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrefresh()\n",
      "File \u001b[1;32mc:\\Users\\tianr\\anaconda3\\Lib\\site-packages\\tqdm\\std.py:1344\u001b[0m, in \u001b[0;36mtqdm.refresh\u001b[1;34m(self, nolock, lock_args)\u001b[0m\n\u001b[0;32m   1342\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1343\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39macquire()\n\u001b[1;32m-> 1344\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdisplay()\n\u001b[0;32m   1345\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m nolock:\n\u001b[0;32m   1346\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\tianr\\anaconda3\\Lib\\site-packages\\tqdm\\std.py:1492\u001b[0m, in \u001b[0;36mtqdm.display\u001b[1;34m(self, msg, pos)\u001b[0m\n\u001b[0;32m   1490\u001b[0m \u001b[39mif\u001b[39;00m pos:\n\u001b[0;32m   1491\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmoveto(pos)\n\u001b[1;32m-> 1492\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msp(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__str__\u001b[39m() \u001b[39mif\u001b[39;00m msg \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m msg)\n\u001b[0;32m   1493\u001b[0m \u001b[39mif\u001b[39;00m pos:\n\u001b[0;32m   1494\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmoveto(\u001b[39m-\u001b[39mpos)\n",
      "File \u001b[1;32mc:\\Users\\tianr\\anaconda3\\Lib\\site-packages\\tqdm\\std.py:347\u001b[0m, in \u001b[0;36mtqdm.status_printer.<locals>.print_status\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m    345\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprint_status\u001b[39m(s):\n\u001b[0;32m    346\u001b[0m     len_s \u001b[39m=\u001b[39m disp_len(s)\n\u001b[1;32m--> 347\u001b[0m     fp_write(\u001b[39m'\u001b[39m\u001b[39m\\r\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m s \u001b[39m+\u001b[39m (\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m \u001b[39m*\u001b[39m \u001b[39mmax\u001b[39m(last_len[\u001b[39m0\u001b[39m] \u001b[39m-\u001b[39m len_s, \u001b[39m0\u001b[39m)))\n\u001b[0;32m    348\u001b[0m     last_len[\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m len_s\n",
      "File \u001b[1;32mc:\\Users\\tianr\\anaconda3\\Lib\\site-packages\\tqdm\\std.py:341\u001b[0m, in \u001b[0;36mtqdm.status_printer.<locals>.fp_write\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m    339\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfp_write\u001b[39m(s):\n\u001b[0;32m    340\u001b[0m     fp\u001b[39m.\u001b[39mwrite(\u001b[39mstr\u001b[39m(s))\n\u001b[1;32m--> 341\u001b[0m     fp_flush()\n",
      "File \u001b[1;32mc:\\Users\\tianr\\anaconda3\\Lib\\site-packages\\tqdm\\utils.py:127\u001b[0m, in \u001b[0;36mDisableOnWriteError.disable_on_exception.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    126\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 127\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    128\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    129\u001b[0m         \u001b[39mif\u001b[39;00m e\u001b[39m.\u001b[39merrno \u001b[39m!=\u001b[39m \u001b[39m5\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\tianr\\anaconda3\\Lib\\site-packages\\ipykernel\\iostream.py:564\u001b[0m, in \u001b[0;36mOutStream.flush\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    562\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpub_thread\u001b[39m.\u001b[39mschedule(evt\u001b[39m.\u001b[39mset)\n\u001b[0;32m    563\u001b[0m     \u001b[39m# and give a timeout to avoid\u001b[39;00m\n\u001b[1;32m--> 564\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m evt\u001b[39m.\u001b[39mwait(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mflush_timeout):\n\u001b[0;32m    565\u001b[0m         \u001b[39m# write directly to __stderr__ instead of warning because\u001b[39;00m\n\u001b[0;32m    566\u001b[0m         \u001b[39m# if this is happening sys.stderr may be the problem.\u001b[39;00m\n\u001b[0;32m    567\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mIOStream.flush timed out\u001b[39m\u001b[39m\"\u001b[39m, file\u001b[39m=\u001b[39msys\u001b[39m.\u001b[39m__stderr__)\n\u001b[0;32m    568\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\tianr\\anaconda3\\Lib\\threading.py:622\u001b[0m, in \u001b[0;36mEvent.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    620\u001b[0m signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flag\n\u001b[0;32m    621\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m signaled:\n\u001b[1;32m--> 622\u001b[0m     signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cond\u001b[39m.\u001b[39mwait(timeout)\n\u001b[0;32m    623\u001b[0m \u001b[39mreturn\u001b[39;00m signaled\n",
      "File \u001b[1;32mc:\\Users\\tianr\\anaconda3\\Lib\\threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    323\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m--> 324\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39macquire(\u001b[39mTrue\u001b[39;00m, timeout)\n\u001b[0;32m    325\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    326\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39macquire(\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.run(train_loader, valid_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
